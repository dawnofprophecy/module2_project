{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e4d50a",
   "metadata": {},
   "source": [
    "\n",
    "# Olist Bronze Ingestion — CSV → DuckDB → Parquet → GCS → BigQuery\n",
    "\n",
    "This notebook ingests the **Olist Brazilian E-commerce** dataset (9 CSVs) by:\n",
    "1. Reading CSVs into **DuckDB**\n",
    "2. Converting to **Parquet**\n",
    "3. Uploading Parquet files to **Google Cloud Storage (GCS)**\n",
    "4. Loading Parquet into **BigQuery native tables** (Bronze dataset)\n",
    "\n",
    "> **Tip:** Run this from a machine with Google Cloud SDK/ADC set up (or attach a Service Account JSON and set `GOOGLE_APPLICATION_CREDENTIALS`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48cb6c",
   "metadata": {},
   "source": [
    "follow step in Coaching2.1 on GCP setup\n",
    "create new project in GCP (project ID: olist-Project-470402), generate a service account for this new project : olist-project-470402-5ef2ea75c5ae.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe5c5f",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Prerequisites\n",
    "\n",
    "- Python 3.9+\n",
    "- Packages:\n",
    "  ```bash\n",
    "  pip install duckdb pyarrow google-cloud-storage google-cloud-bigquery google-cloud-bigquery-storage tqdm\n",
    "  ```\n",
    "- Google Cloud:\n",
    "  - A GCS bucket (e.g., `gs://YOUR_BUCKET/olist/bronze/parquet/`) create GCS bucket follow 2. Using the Google Cloud Console (UI) below\n",
    "  - A BigQuery dataset (e.g., `olist_bronze`), or permission to create one\n",
    "  - Application Default Credentials (ADC) or a Service Account JSON:\n",
    "    ```bash\n",
    "    #gcloud auth login\n",
    "    #login to the new project id. \n",
    "    # or set env var to your SA key:\n",
    "    export GOOGLE_APPLICATION_CREDENTIALS=/path/to/sa.json\n",
    "    ```\n",
    "- Olist CSVs downloaded locally in a folder (e.g., `./data/olist/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cdfc796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=FBqK3c7iUhZDLD34tky5j0qLApAgle&access_type=offline&code_challenge=wbPEhqRASDKj6SQYVOLfH1UjJo8Ob52WWJO1t3mMXvU&code_challenge_method=S256\n",
      "\n",
      "^C\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Run these in a notebook cell using ! or %%bash\n",
    "!gcloud auth login\n",
    "!gcloud config set project olist-project-470402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb2a3f",
   "metadata": {},
   "source": [
    " Using the Google Cloud Console (UI)\n",
    "\t1.\tGo to: https://console.cloud.google.com/storage\n",
    "\t2.\tselect olist project, then Click “Create bucket”\n",
    "\t3.\tEnter a globally unique name olist_bronze_data_12345)\n",
    "\t4.\tChoose:\n",
    "\t•\tLocation: US / multi-region / region close to you\n",
    "\t•\tStorage class: STANDARD (default)\n",
    "\t•\tAccess control: Uniform bucket-level access (recommended)\n",
    "\t5.\tClick Create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba2457",
   "metadata": {},
   "source": [
    "## 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449a9c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('orders', 'olist_orders_dataset.csv'),\n",
       " ('order_items', 'olist_order_items_dataset.csv'),\n",
       " ('payments', 'olist_order_payments_dataset.csv'),\n",
       " ('reviews', 'olist_order_reviews_dataset.csv'),\n",
       " ('customers', 'olist_customers_dataset.csv'),\n",
       " ('sellers', 'olist_sellers_dataset.csv'),\n",
       " ('products', 'olist_products_dataset.csv'),\n",
       " ('geolocation', 'olist_geolocation_dataset.csv'),\n",
       " ('product_category_name_translation',\n",
       "  'product_category_name_translation.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# === Local paths ===\n",
    "BASE_DIR = Path(\"./olist_data\")   # change to your local folder with CSVs\n",
    "PARQUET_DIR = Path(\"./data/parquet/olist\")  # where Parquet will be written\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === GCP ===\n",
    "GCP_PROJECT = \"olist-project-470402\"\n",
    "BQ_DATASET  = \"olist_bronze\"             # BigQuery dataset name\n",
    "GCS_BUCKET  = \"olist_bronze_data_12345\"     # without gs://\n",
    "GCS_PREFIX  = \"olist/bronze/parquet\"     # path prefix inside bucket\n",
    "\n",
    "# Table mapping: {logical_table_name: csv_filename}\n",
    "OLIST_FILES = {\n",
    "    \"orders\": \"olist_orders_dataset.csv\",\n",
    "    \"order_items\": \"olist_order_items_dataset.csv\",\n",
    "    \"payments\": \"olist_order_payments_dataset.csv\",\n",
    "    \"reviews\": \"olist_order_reviews_dataset.csv\",\n",
    "    \"customers\": \"olist_customers_dataset.csv\",\n",
    "    \"sellers\": \"olist_sellers_dataset.csv\",\n",
    "    \"products\": \"olist_products_dataset.csv\",\n",
    "    \"geolocation\": \"olist_geolocation_dataset.csv\",\n",
    "    \"product_category_name_translation\": \"product_category_name_translation.csv\",\n",
    "}\n",
    "\n",
    "list(OLIST_FILES.items())[:9]  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2d6a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('olist_data/olist_sellers_dataset.csv'), PosixPath('olist_data/product_category_name_translation.csv'), PosixPath('olist_data/olist_orders_dataset.csv'), PosixPath('olist_data/olist_order_items_dataset.csv'), PosixPath('olist_data/olist_customers_dataset.csv'), PosixPath('olist_data/olist_geolocation_dataset.csv'), PosixPath('olist_data/olist_order_payments_dataset.csv'), PosixPath('olist_data/olist_order_reviews_dataset.csv'), PosixPath('olist_data/olist_products_dataset.csv')]\n"
     ]
    }
   ],
   "source": [
    "print(list(BASE_DIR.glob(\"*.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c377e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod -R u+rw ./olist_data\n",
    "!chmod -R u+rw ./data/parquet/olist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555c29d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convert CSV → Parquet with DuckDB\n",
    "\n",
    "We use `read_csv_auto` for schema inference and write **Parquet** for efficient loading into BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files written to: data/parquet/olist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%pip install tqdm\n",
    "\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "con = duckdb.connect(\"olist.duckdb\")\n",
    "con.execute(\"CREATE SCHEMA IF NOT EXISTS bronze; SET schema=bronze;\")\n",
    "\n",
    "for tbl, csv in tqdm(OLIST_FILES.items()):\n",
    "    csv_path = (BASE_DIR / csv).as_posix()\n",
    "    pq_path  = (PARQUET_DIR / f\"{tbl}.parquet\").as_posix()\n",
    "    sql = f\"\"\"\n",
    "    COPY (\n",
    "      SELECT * FROM read_csv_auto('{csv_path}', header=True)\n",
    "    ) TO '{pq_path}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    con.execute(sql)\n",
    "\n",
    "print(\"Parquet files written to:\", PARQUET_DIR.as_posix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e23800",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Upload Parquet to GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ca8e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:21<00:00,  2.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'orders': 'gs://olist_bronze_data_12345/olist/bronze/parquet/orders.parquet',\n",
       " 'order_items': 'gs://olist_bronze_data_12345/olist/bronze/parquet/order_items.parquet',\n",
       " 'payments': 'gs://olist_bronze_data_12345/olist/bronze/parquet/payments.parquet',\n",
       " 'reviews': 'gs://olist_bronze_data_12345/olist/bronze/parquet/reviews.parquet',\n",
       " 'customers': 'gs://olist_bronze_data_12345/olist/bronze/parquet/customers.parquet',\n",
       " 'sellers': 'gs://olist_bronze_data_12345/olist/bronze/parquet/sellers.parquet',\n",
       " 'products': 'gs://olist_bronze_data_12345/olist/bronze/parquet/products.parquet',\n",
       " 'geolocation': 'gs://olist_bronze_data_12345/olist/bronze/parquet/geolocation.parquet',\n",
       " 'product_category_name_translation': 'gs://olist_bronze_data_12345/olist/bronze/parquet/product_category_name_translation.parquet'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#%pip install google-cloud-storage\n",
    "\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = storage.Client(project=GCP_PROJECT)\n",
    "bucket = client.bucket(GCS_BUCKET)\n",
    "\n",
    "gcs_uris = {}\n",
    "for tbl in tqdm(OLIST_FILES.keys()):\n",
    "    local_path = PARQUET_DIR / f\"{tbl}.parquet\"\n",
    "    blob_path = f\"{GCS_PREFIX}/{tbl}.parquet\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_filename(local_path.as_posix())\n",
    "    gcs_uri = f\"gs://{GCS_BUCKET}/{blob_path}\"\n",
    "    gcs_uris[tbl] = gcs_uri\n",
    "\n",
    "gcs_uris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d3d65",
   "metadata": {},
   "source": [
    "at this step, can check GCS, all 9 parquet should be in the bucket (olist_bronze_data_1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64669c4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Load Parquet into BigQuery native tables (Bronze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f1f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset olist_bronze may already exist: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/olist-project-470402/datasets?prettyPrint=false: Already Exists: Dataset olist-project-470402:olist_bronze\n",
      "Loaded 99441 rows into olist-project-470402.olist_bronze.orders\n",
      "Loaded 112650 rows into olist-project-470402.olist_bronze.order_items\n",
      "Loaded 103886 rows into olist-project-470402.olist_bronze.payments\n",
      "Loaded 99224 rows into olist-project-470402.olist_bronze.reviews\n",
      "Loaded 99441 rows into olist-project-470402.olist_bronze.customers\n",
      "Loaded 3095 rows into olist-project-470402.olist_bronze.sellers\n",
      "Loaded 32951 rows into olist-project-470402.olist_bronze.products\n",
      "Loaded 1000163 rows into olist-project-470402.olist_bronze.geolocation\n",
      "Loaded 71 rows into olist-project-470402.olist_bronze.product_category_name_translation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%pip install google-cloud-bigquery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "bq = bigquery.Client(project=GCP_PROJECT)\n",
    "\n",
    "# Create dataset if not exists\n",
    "dataset_ref = bigquery.Dataset(f\"{GCP_PROJECT}.{BQ_DATASET}\")\n",
    "dataset_ref.location = \"US\"   # change region if needed\n",
    "try:\n",
    "    bq.create_dataset(dataset_ref)\n",
    "    print(f\"Created dataset {BQ_DATASET}\")\n",
    "except Exception as e:\n",
    "    print(f\"Dataset {BQ_DATASET} may already exist:\", e)\n",
    "\n",
    "# Load each Parquet into a native BQ table\n",
    "job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.PARQUET, write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "for tbl, uri in gcs_uris.items():\n",
    "    table_id = f\"{GCP_PROJECT}.{BQ_DATASET}.{tbl}\"\n",
    "    load_job = bq.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    load_job.result()\n",
    "    table = bq.get_table(table_id)\n",
    "    print(f\"Loaded {table.num_rows} rows into {table_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd01cf4",
   "metadata": {},
   "source": [
    "after run this step, check BQ olist-project, olist_bronze dataset. all 9 dataset are loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de5892",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Quick verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c96bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: db-dtypes in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from db-dtypes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=24.2.0 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from db-dtypes) (24.2)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from db-dtypes) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=13.0.0 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from db-dtypes) (21.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from pandas>=1.5.3->db-dtypes) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from pandas>=1.5.3->db-dtypes) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pds/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.3->db-dtypes) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pds/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders                    99,441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pds/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_items               112,650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pds/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payments                  103,886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pds/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews                   99,224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pds/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers                 99,441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install db-dtypes\n",
    "\n",
    "# Example: check counts of a few key tables\n",
    "for tbl in [\"orders\", \"order_items\", \"payments\", \"reviews\", \"customers\"]:\n",
    "    table_id = f\"{GCP_PROJECT}.{BQ_DATASET}.{tbl}\"\n",
    "    rows = bq.query(f\"SELECT COUNT(*) AS cnt FROM `{table_id}`\").result().to_dataframe()[\"cnt\"][0]\n",
    "    print(f\"{tbl:25s} {rows:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54665581",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Notes & Tips\n",
    "\n",
    "- **Schema control**: If you want strict types, you can cast in DuckDB before writing Parquet.\n",
    "- **Partitioning**: For large data, write partitioned Parquet (e.g., by year/month) and use partitioned BQ tables.\n",
    "- **Idempotency**: Set `write_disposition` to `WRITE_APPEND` and include load metadata to support incremental loads.\n",
    "- **Costs**: BigQuery charges on storage (GB-month) and query (bytes scanned). Parquet reduces both.\n",
    "- **Security**: Use a dedicated service account with least-privilege access to GCS and BigQuery.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
